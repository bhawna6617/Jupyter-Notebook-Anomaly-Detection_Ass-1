{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e3047c",
   "metadata": {},
   "source": [
    "# question 1:- What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection is a technique used in data analysis to identify unusual patterns, behaviors, or observations that deviate significantly from the majority of the data. These deviations, known as anomalies or outliers, can indicate important events, errors, or changes in the underlying system being monitored.\n",
    "\n",
    "# Purpose of Anomaly Detection\n",
    "# Detecting Fraud:\n",
    "\n",
    "# In financial transactions, anomaly detection can identify suspicious activities that may indicate fraudulent behavior, such as unauthorized transactions, unusual spending patterns, or account takeovers.\n",
    "# Monitoring Systems and Networks:\n",
    "\n",
    "# In IT and network security, anomaly detection can identify unusual patterns of network traffic, system behavior, or user activities that may indicate security breaches, attacks, or system failures.\n",
    "# Quality Control:\n",
    "\n",
    "# In manufacturing, anomaly detection can monitor production processes to identify defects, equipment malfunctions, or deviations from standard operating procedures, ensuring product quality and operational efficiency.\n",
    "# Health Monitoring:\n",
    "\n",
    "# In healthcare, anomaly detection can monitor patient data, such as vital signs or laboratory results, to detect early signs of medical conditions, irregularities, or emergencies.\n",
    "# Fault Detection in Equipment:\n",
    "\n",
    "# In industrial applications, anomaly detection can be used for predictive maintenance by identifying early signs of equipment failure or degradation, allowing for timely repairs and reducing downtime.\n",
    "# Techniques for Anomaly Detection\n",
    "# Statistical Methods:\n",
    "\n",
    "# Use statistical models to identify data points that deviate significantly from the expected distribution. Examples include z-scores, Grubbs' test, and the Mahalanobis distance.\n",
    "# Machine Learning Algorithms:\n",
    "\n",
    "# Supervised Learning: Algorithms like Support Vector Machines (SVM), Neural Networks, and decision trees can be trained on labeled data to identify anomalies.\n",
    "# Unsupervised Learning: Algorithms like k-means clustering, DBSCAN, and autoencoders can detect anomalies without labeled data by identifying patterns and deviations within the data.\n",
    "# Distance-Based Methods:\n",
    "\n",
    "# Measure the distance between data points in feature space. Points that are far from others are considered anomalies. Examples include k-nearest neighbors (k-NN) and Local Outlier Factor (LOF).\n",
    "# Density-Based Methods:\n",
    "\n",
    "# Identify regions of high and low data density. Data points in low-density regions are considered anomalies. Examples include DBSCAN and LOF.\n",
    "# Domain-Specific Techniques:\n",
    "\n",
    "# Custom methods designed for specific applications or data types, incorporating domain knowledge and specific characteristics of the data.\n",
    "# Challenges in Anomaly Detection\n",
    "# Imbalanced Data:\n",
    "\n",
    "# Anomalies are often rare compared to normal observations, leading to highly imbalanced datasets that can be challenging to analyze effectively.\n",
    "# High Dimensionality:\n",
    "\n",
    "# In datasets with many features, detecting anomalies can be difficult due to the curse of dimensionality, which complicates the identification of meaningful patterns and outliers.\n",
    "# Dynamic Data:\n",
    "\n",
    "# In systems where data patterns change over time, static models may fail to detect anomalies, requiring adaptive or real-time detection methods.\n",
    "# False Positives and Negatives:\n",
    "\n",
    "# Balancing the trade-off between false positives (normal data points incorrectly flagged as anomalies) and false negatives (anomalies missed by the detection algorithm) is critical to ensure accurate and reliable anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c303447",
   "metadata": {},
   "source": [
    "# question 2:- What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af358312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection faces several key challenges that can affect the accuracy and reliability of the detection process. These challenges include:\n",
    "\n",
    "# 1. Imbalanced Data\n",
    "# Challenge: Anomalies are often rare compared to normal data points, leading to highly imbalanced datasets. This imbalance can make it difficult for anomaly detection models to learn and accurately identify anomalies.\n",
    "# Solution: Techniques like resampling (oversampling the minority class or undersampling the majority class), using anomaly-specific evaluation metrics (e.g., precision, recall, F1-score), and employing specialized algorithms designed to handle imbalanced data can help mitigate this challenge.\n",
    "# 2. High Dimensionality\n",
    "# Challenge: Datasets with many features can complicate the anomaly detection process due to the curse of dimensionality, where the distance between data points becomes less meaningful, and the data sparsity increases.\n",
    "# Solution: Dimensionality reduction techniques such as Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), or Autoencoders can be used to reduce the number of features while retaining the essential characteristics of the data.\n",
    "# 3. Dynamic and Evolving Data\n",
    "# Challenge: In many real-world applications, data patterns change over time, making it challenging for static anomaly detection models to remain effective. This is common in network security, financial markets, and industrial monitoring.\n",
    "# Solution: Implementing adaptive or online learning algorithms that can update their models as new data arrives can help address this issue. Techniques like sliding windows, recurrent neural networks (RNNs), or incremental learning models can be useful in such scenarios.\n",
    "# 4. Noise and Outliers\n",
    "# Challenge: Real-world data often contains noise and outliers that are not true anomalies but can confuse detection algorithms. Distinguishing between noise and genuine anomalies can be difficult.\n",
    "# Solution: Preprocessing steps such as data cleaning, noise reduction, and robust statistical methods can help reduce the impact of noise. Robust anomaly detection algorithms that can tolerate a certain level of noise are also beneficial.\n",
    "# 5. Lack of Labeled Data\n",
    "# Challenge: In many cases, especially in unsupervised learning scenarios, there is a lack of labeled data indicating which points are anomalies. This makes it hard to train and evaluate models.\n",
    "# Solution: Semi-supervised and unsupervised learning techniques, where the model learns from the structure and distribution of the data, can be used. Additionally, techniques like active learning, where the model queries an oracle (e.g., a human expert) for labels on uncertain points, can also help.\n",
    "# 6. Scalability\n",
    "# Challenge: Analyzing large-scale datasets efficiently is a significant challenge due to computational and memory constraints.\n",
    "# Solution: Leveraging distributed computing frameworks (e.g., Apache Spark), optimizing algorithms for scalability, and using approximate methods for large-scale data can help in scaling anomaly detection processes.\n",
    "# 7. Interpretability\n",
    "# Challenge: Understanding and interpreting why a particular data point is flagged as an anomaly can be difficult, especially with complex models like neural networks.\n",
    "# Solution: Using simpler and more interpretable models, or applying model-agnostic interpretability techniques (e.g., SHAP values, LIME) to explain the decisions of complex models, can aid in making the results more understandable.\n",
    "# 8. Real-time Detection\n",
    "# Challenge: In applications like network security or fraud detection, anomalies need to be detected and responded to in real-time, which requires low-latency processing and quick decision-making.\n",
    "# Solution: Implementing real-time data processing frameworks and optimizing algorithms for low-latency execution can address this need. Techniques like stream processing and event-driven architectures are often employed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c4be6",
   "metadata": {},
   "source": [
    "# question 3:- How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ddc651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised and supervised anomaly detection are two different approaches to identifying anomalies in data. Here are the key differences between them:\n",
    "\n",
    "# Supervised Anomaly Detection\n",
    "# Definition: Supervised anomaly detection involves training a model on labeled data, where each data point is explicitly marked as either normal or anomalous.\n",
    "\n",
    "# Training Data:\n",
    "\n",
    "# Labeled Data: Requires a labeled dataset with known normal and anomalous instances.\n",
    "# Training Process: The model learns to distinguish between normal and anomalous patterns based on the provided labels.\n",
    "# Techniques:\n",
    "\n",
    "# Classification Algorithms: Methods like decision trees, support vector machines (SVM), neural networks, and ensemble methods (e.g., Random Forest) are commonly used.\n",
    "# Evaluation Metrics: Accuracy, precision, recall, F1-score, and ROC-AUC are typical metrics used to evaluate performance.\n",
    "# Advantages:\n",
    "\n",
    "# Accuracy: Can achieve high accuracy if a sufficiently large and representative labeled dataset is available.\n",
    "# Interpretability: The decision boundaries and rules can be easier to interpret with some algorithms.\n",
    "# Disadvantages:\n",
    "\n",
    "# Dependency on Labeled Data: Requires labeled data, which can be expensive and time-consuming to obtain.\n",
    "# Limited Adaptability: May not generalize well to new, unseen types of anomalies that were not present in the training data.\n",
    "# Unsupervised Anomaly Detection\n",
    "# Definition: Unsupervised anomaly detection does not require labeled data. It identifies anomalies based on the inherent properties and patterns in the data.\n",
    "\n",
    "# Training Data:\n",
    "\n",
    "# Unlabeled Data: Operates on unlabeled data, detecting anomalies based on deviations from the normal patterns.\n",
    "# Training Process: The model clusters or models the data distribution and identifies points that deviate significantly from the expected pattern.\n",
    "# Techniques:\n",
    "\n",
    "# Clustering Methods: Techniques like k-means, DBSCAN, and hierarchical clustering can be used to detect anomalies as points that do not fit well into any cluster.\n",
    "# Statistical Methods: Methods like Gaussian mixture models (GMM), z-scores, and isolation forests.\n",
    "# Density-Based Methods: Techniques like Local Outlier Factor (LOF) that identify anomalies based on the density of data points in the feature space.\n",
    "# Advantages:\n",
    "\n",
    "# No Need for Labels: Can be applied to datasets where labeled data is unavailable or difficult to obtain.\n",
    "# Flexibility: Can detect novel and previously unseen anomalies because it does not rely on pre-defined labels.\n",
    "# Disadvantages:\n",
    "\n",
    "# Lower Accuracy: May have lower accuracy compared to supervised methods if the normal and anomalous patterns are not well separated.\n",
    "# Parameter Sensitivity: Performance can be sensitive to the choice of parameters (e.g., number of clusters, distance thresholds) and may require extensive tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c71d7",
   "metadata": {},
   "source": [
    "# question 4:- What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c99777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection algorithms can be categorized based on their underlying principles and the nature of the data they handle. Here are the main categories:\n",
    "\n",
    "# 1. Statistical Methods\n",
    "# Description: These methods rely on statistical models to identify anomalies as data points that deviate significantly from the expected distribution.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Gaussian Distribution: Assumes data follows a Gaussian distribution and uses z-scores or confidence intervals to identify anomalies.\n",
    "# Mahalanobis Distance: Measures the distance of a point from the mean of a multivariate distribution.\n",
    "# Grubbs' Test: Detects outliers in a dataset that is assumed to follow a normal distribution.\n",
    "# Advantages: Simple and effective for data that follows known statistical distributions.\n",
    "\n",
    "# Disadvantages: Assumes underlying data distribution, which may not hold true for all datasets.\n",
    "\n",
    "# 2. Machine Learning Algorithms\n",
    "# Supervised Methods: Requires labeled training data with known normal and anomalous instances.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Support Vector Machines (SVM): Classifies data points based on labeled training data.\n",
    "# Neural Networks: Deep learning models that can learn complex patterns from labeled data.\n",
    "# Random Forests: Ensemble method that can classify anomalies based on labeled training data.\n",
    "# Unsupervised Methods: Does not require labeled data and identifies anomalies based on patterns within the data.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# k-Means Clustering: Identifies anomalies as points that do not fit well into any cluster.\n",
    "# DBSCAN: Density-based clustering that identifies anomalies as points in low-density regions.\n",
    "# Autoencoders: Neural network-based method that learns to reconstruct normal data and identifies anomalies as data with high reconstruction error.\n",
    "# Advantages: Can handle complex and high-dimensional data.\n",
    "\n",
    "# Disadvantages: Supervised methods require labeled data; unsupervised methods can be computationally intensive.\n",
    "\n",
    "# 3. Distance-Based Methods\n",
    "# Description: These methods identify anomalies based on the distance between data points in feature space.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# k-Nearest Neighbors (k-NN): Identifies anomalies as points that are far from their nearest neighbors.\n",
    "# Local Outlier Factor (LOF): Measures the local density deviation of a data point with respect to its neighbors.\n",
    "# Advantages: Simple to implement and understand.\n",
    "\n",
    "# Disadvantages: Can be computationally expensive for large datasets; performance can degrade with high-dimensional data.\n",
    "\n",
    "# 4. Density-Based Methods\n",
    "# Description: These methods identify anomalies as points that lie in low-density regions of the data space.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Clusters data points and identifies anomalies as points in low-density regions.\n",
    "# Local Outlier Factor (LOF): Compares the density of a point to the density of its neighbors to identify anomalies.\n",
    "# Advantages: Effective for datasets with varying densities.\n",
    "\n",
    "# Disadvantages: Sensitive to parameter choices (e.g., epsilon in DBSCAN).\n",
    "\n",
    "# 5. Model-Based Methods\n",
    "# Description: These methods build a model of the normal behavior of the data and identify anomalies as deviations from this model.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Gaussian Mixture Models (GMM): Models data as a mixture of multiple Gaussian distributions and identifies anomalies as points with low likelihood under the model.\n",
    "# Hidden Markov Models (HMM): Models sequential data and identifies anomalies as sequences that deviate from the learned model.\n",
    "# Advantages: Can capture complex patterns and dependencies in the data.\n",
    "\n",
    "# Disadvantages: Requires assumptions about the underlying data distribution or process.\n",
    "\n",
    "# 6. Ensemble Methods\n",
    "# Description: These methods combine multiple anomaly detection techniques to improve robustness and accuracy.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Isolation Forests: Constructs an ensemble of trees to isolate anomalies.\n",
    "# Voting-Based Methods: Combine the results of different anomaly detection algorithms to make a final decision.\n",
    "# Advantages: Can leverage the strengths of different methods and improve overall performance.\n",
    "\n",
    "# Disadvantages: Can be more complex and computationally expensive.\n",
    "\n",
    "# 7. Domain-Specific Methods\n",
    "# Description: Customized methods designed for specific applications or types of data, incorporating domain knowledge and specific characteristics.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Rule-Based Systems: Use domain-specific rules to identify anomalies (e.g., thresholds for sensor data).\n",
    "# Graph-Based Methods: Identify anomalies in graph data by analyzing node or edge properties.\n",
    "# Advantages: Tailored to specific applications, potentially leading to higher accuracy.\n",
    "\n",
    "# Disadvantages: May not generalize well to other domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e542b",
   "metadata": {},
   "source": [
    "# question 5:-What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e386b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance-based anomaly detection methods rely on several key assumptions about the data and the nature of anomalies. Understanding these assumptions is crucial for effectively applying these methods and interpreting their results. Here are the main assumptions:\n",
    "\n",
    "# 1. Distance Metrics are Meaningful\n",
    "# Assumption: The chosen distance metric (e.g., Euclidean, Manhattan, Mahalanobis) accurately reflects the similarity or dissimilarity between data points.\n",
    "# Implication: Points that are close in the feature space are similar, while points that are far apart are dissimilar. This assumes that the feature space is well-defined and that distances in this space are meaningful for the given data.\n",
    "# 2. Homogeneity of Normal Data\n",
    "# Assumption: Normal data points are densely packed or form a coherent cluster(s) in the feature space.\n",
    "# Implication: Anomalies are points that lie far from these dense regions or clusters. The method assumes that the normal data distribution is relatively homogeneous and does not contain significant subclusters with different densities.\n",
    "# 3. Sparsity of Anomalies\n",
    "# Assumption: Anomalies are few and far between compared to normal data points.\n",
    "# Implication: Anomalies are identified as data points that do not have many neighbors within a certain distance or that have a significantly lower local density compared to normal points.\n",
    "# 4. Consistency of Feature Scale\n",
    "# Assumption: All features contribute equally to the distance metric, or appropriate scaling/normalization has been applied.\n",
    "# Implication: If features have different scales or variances, distance calculations can be dominated by features with larger scales, leading to incorrect anomaly detection. Proper feature scaling or normalization (e.g., z-score normalization, min-max scaling) is assumed to be in place.\n",
    "# 5. Independence of Features (for Simple Metrics)\n",
    "# Assumption: In some distance metrics (like Euclidean distance), it is assumed that features are independent and contribute equally to the distance measure.\n",
    "# Implication: This assumption may not hold true for all datasets, particularly those with correlated features. In such cases, using metrics that account for feature correlations (e.g., Mahalanobis distance) can be more appropriate.\n",
    "# 6. Stationarity of Data (for Temporal Data)\n",
    "# Assumption: For temporal or sequential data, it is often assumed that the statistical properties of the data do not change over time.\n",
    "# Implication: Anomalies are detected based on the current structure of the data, assuming that past patterns are indicative of future patterns. This may not hold in non-stationary environments where the data distribution changes over time.\n",
    "# Common Distance-Based Anomaly Detection Methods\n",
    "# k-Nearest Neighbors (k-NN) for Anomaly Detection:\n",
    "\n",
    "# Measures the distance of a point to its k-nearest neighbors.\n",
    "# Points with large average distances to their k-nearest neighbors are considered anomalies.\n",
    "# Local Outlier Factor (LOF):\n",
    "\n",
    "# Compares the local density of a point to the local densities of its neighbors.\n",
    "# Points with significantly lower local densities compared to their neighbors are considered anomalies.\n",
    "# Distance to the Nearest Neighbor (Single-Linkage):\n",
    "\n",
    "# Measures the distance to the closest data point.\n",
    "# Points with unusually large distances to their nearest neighbor are flagged as anomalies.\n",
    "# Limitations and Considerations\n",
    "# Curse of Dimensionality: In high-dimensional spaces, distances between points can become less meaningful due to the sparsity of data. Dimensionality reduction techniques (e.g., PCA, t-SNE) may be required.\n",
    "# Parameter Sensitivity: Methods like k-NN and LOF require careful tuning of parameters (e.g., the number of neighbors k, distance threshold), which can significantly affect performance.\n",
    "# Computational Complexity: Distance calculations can be computationally expensive for large datasets, requiring optimization techniques or approximate methods to improve efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b9204",
   "metadata": {},
   "source": [
    "# question 6:-How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d1c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Local Outlier Factor (LOF) algorithm computes anomaly scores by measuring the local density deviation of a data point with respect to its neighbors. The core idea is that anomalies are points that have a significantly lower density compared to their neighbors. Here’s a step-by-step explanation of how the LOF algorithm computes anomaly scores:\n",
    "\n",
    "# Step-by-Step Computation of LOF Scores\n",
    "# Compute k-Distance and k-Distance Neighbors:\n",
    "\n",
    "# k-Distance: For each data point \n",
    "# 𝑝\n",
    "# p, determine the distance to its \n",
    "# 𝑘\n",
    "# k-th nearest neighbor. This distance is called the \n",
    "# 𝑘\n",
    "# k-distance of \n",
    "# 𝑝\n",
    "# p and denoted as \n",
    "\n",
    "# k-distance(p).\n",
    "# k-Distance Neighborhood: The set of points within the \n",
    "\n",
    "# k-distance of \n",
    "\n",
    "# p is called the \n",
    "\n",
    "# k-distance neighborhood of \n",
    "# 𝑝\n",
    "# p and denoted as \n",
    "\n",
    "#  (p).\n",
    "# Compute Reachability Distance:\n",
    "\n",
    "# Reachability Distance: For a data point \n",
    "# 𝑝\n",
    "# p and a neighbor \n",
    "# 𝑜\n",
    "# o, the reachability distance is defined as:\n",
    "# reachability_distance\n",
    "\n",
    "#  (p,o)=max(k-distance(o),distance(p,o))\n",
    "# This distance ensures that points within the dense cluster have smaller reachability distances.\n",
    "# Compute Local Reachability Density (LRD):\n",
    "\n",
    "# Local Reachability Density: For a data point \n",
    "# 𝑝\n",
    "# p, the local reachability density is the inverse of the average reachability distance based on the \n",
    "# 𝑘\n",
    "# k-distance neighborhood:\n",
    "# LRD\n",
    "# reachability_distance\n",
    "\n",
    " \n",
    "# The LRD represents the density around the point \n",
    "# 𝑝\n",
    "# p.\n",
    "# Compute Local Outlier Factor (LOF):\n",
    "\n",
    "# Local Outlier Factor: For a data point \n",
    "# 𝑝\n",
    "# p, the LOF score is the average of the ratios of the local reachability density of \n",
    "# 𝑝\n",
    "# p and those of \n",
    "# 𝑝\n",
    "# p's \n",
    "# 𝑘\n",
    "# k-distance neighbors:\n",
    "# LOF\n",
    "\n",
    "# The LOF score indicates how much the density around \n",
    "# 𝑝\n",
    "# p differs from the densities around its neighbors.\n",
    "# Interpretation of LOF Scores\n",
    "# LOF Score ≈ 1: The point has a density similar to its neighbors and is likely a normal point.\n",
    "# LOF Score > 1: The point has a lower density compared to its neighbors, indicating it is an outlier. The higher the LOF score, the more likely the point is an anomaly.\n",
    "# LOF Score < 1: The point has a higher density compared to its neighbors, which might be rare but is not typically considered anomalous in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6f70b",
   "metadata": {},
   "source": [
    "# question 7:-What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a01c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Isolation Forest algorithm, which is used for anomaly detection, has several key parameters that can affect its performance and behavior. Understanding these parameters is essential for effectively applying the algorithm to different datasets. Here are the main parameters of the Isolation Forest algorithm:\n",
    "\n",
    "# n_estimators:\n",
    "\n",
    "# Definition: Number of base estimators (individual isolation trees) to use in the ensemble.\n",
    "# Impact: Increasing the number of estimators generally improves the performance of the Isolation Forest, as it provides a more robust estimate of anomaly scores. However, it also increases computation time.\n",
    "# max_samples:\n",
    "\n",
    "# Definition: Number of samples to draw from the dataset to build each individual tree.\n",
    "# Impact: Controlling the number of samples used in each tree affects the randomness and diversity of the ensemble. Higher values can improve the accuracy of the isolation trees but may increase computational overhead.\n",
    "# contamination:\n",
    "\n",
    "# Definition: Expected proportion of anomalies in the dataset.\n",
    "# Impact: Helps adjust the threshold for deciding which instances are anomalies. Typically, this parameter is set based on domain knowledge or preliminary analysis of the dataset. It influences the decision boundary for anomaly scores.\n",
    "# max_features:\n",
    "\n",
    "# Definition: Number of features to consider when splitting nodes.\n",
    "# Impact: Controls the randomness of each isolation tree. A smaller value reduces overfitting but may also decrease the ability of the algorithm to capture complex relationships in high-dimensional data.\n",
    "# bootstrap:\n",
    "\n",
    "# Definition: Whether to use bootstrap sampling when building trees.\n",
    "# Impact: Similar to other ensemble methods, bootstrap sampling introduces randomness and helps improve the diversity of individual trees in the forest. Setting it to True enables bootstrap sampling, which is typical for ensemble learning methods.\n",
    "# random_state:\n",
    "\n",
    "# Definition: Seed for the random number generator.\n",
    "# Impact: Ensures reproducibility of results. By setting a specific random state, you can reproduce the same results across different runs of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f9132",
   "metadata": {},
   "source": [
    "# question:-8 If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To compute the anomaly score of a data point using the k-Nearest Neighbors (k-NN) algorithm for anomaly detection, we typically follow these steps:\n",
    "\n",
    "# Calculate the k-Distance: Find the distance to the \n",
    "# 𝑘\n",
    "# k-th nearest neighbor of the data point. This distance is denoted as \n",
    "\n",
    "# k-distance(p).\n",
    "\n",
    "# Find the k-Nearest Neighbors: Identify the \n",
    "# 𝑘\n",
    "# k nearest neighbors of the data point within the dataset.\n",
    "\n",
    "# Compute the Reachability Distance: For each neighbor \n",
    "# 𝑜\n",
    "# o within the \n",
    "# 𝑘\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
